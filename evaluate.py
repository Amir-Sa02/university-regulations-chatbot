import os
from dotenv import load_dotenv
from core_components import retriever, llm
from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator

# --- Load environment variables ---
load_dotenv()

def main():
    """
    Main function to run the comprehensive evaluation process.
    """
    print("ğŸš€ Starting RAG System Evaluation (Official Documentation Method)...")

    if retriever is None or llm is None:
        print("âŒ Core RAG components not initialized. Please check core_components.py.")
        return

    # --- 1. Define our Golden Evaluation Dataset ---
    eval_dataset = [
        {
            "query": "Ø­Ø¯Ø§Ù‚Ù„ Ù†Ù…Ø±Ù‡ Ù‚Ø¨ÙˆÙ„ÛŒ Ø¯Ø± Ù‡Ø± Ø¯Ø±Ø³ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ",
            "expected_context_substring": "Ø­Ø¯Ø§Ù‚Ù„ Ù†Ù…Ø±Ù‡ Ù‚Ø¨ÙˆÙ„ÛŒ Ø¯Ø± Ù‡Ø± Ø¯Ø±Ø³ Û±Û° Ø§Ø³Øª"
        },
        {
            "query": "Ù…Ø¯Øª Ù…Ø¬Ø§Ø² Ù…Ø±Ø®ØµÛŒ Ø²Ø§ÛŒÙ…Ø§Ù† Ú†Ù†Ø¯ Ù†ÛŒÙ…Ø³Ø§Ù„ Ø§Ø³ØªØŸ",
            "expected_context_substring": "Ù…Ø¯Øª Ù…Ø¬Ø§Ø² Ù…Ø±Ø®ØµÛŒ Ø²Ø§ÛŒÙ…Ø§Ù†ØŒ Ø¯Ùˆ Ù†ÛŒÙ…Ø³Ø§Ù„ ØªØ­ØµÛŒÙ„ÛŒ"
        },
        {
            "query": "Ø§Ú¯Ø± Ø¯Ø§Ù†Ø´Ø¬Ùˆ Ù…Ø´Ø±ÙˆØ· Ø´ÙˆØ¯ØŒ Ø¯Ø± Ù†ÛŒÙ…Ø³Ø§Ù„ Ø¨Ø¹Ø¯ Ø­Ø¯Ø§Ú©Ø«Ø± Ú†Ù†Ø¯ ÙˆØ§Ø­Ø¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø®Ø° Ú©Ù†Ø¯ØŸ",
            "expected_context_substring": "Ø­Ø¯Ø§Ú©Ø«Ø± Ù…ÛŒØªÙˆØ§Ù†Ø¯ ØªØ§ Û±Û´ ÙˆØ§Ø­Ø¯ Ø¯Ø±Ø³ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†Ø¯"
        },
        {
            "query": "Ø¢ÛŒØ§ ØªØºÛŒÛŒØ± Ø±Ø´ØªÙ‡ Ø§Ø² Ø¯ÙˆØ±Ù‡ Ø´Ø¨Ø§Ù†Ù‡ Ø¨Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ù…Ø¬Ø§Ø² Ø§Ø³ØªØŸ",
            "expected_context_substring": "Ø§Ø² Ø´Ø¨Ø§Ù†Ù‡ Ø¨Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡ ØŒ Ø§Ø² ØºÛŒØ± Ø­Ø¶ÙˆØ±ÛŒ Ø¨Ù‡ Ø­Ø¶ÙˆØ±ÛŒ Ùˆ Ù†ÛŒÙ…Ù‡ Ø­Ø¶ÙˆØ±ÛŒ Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª"
        },
        {
            "query": "Ù¾Ø§ÛŒØªØ®Øª Ø§ÛŒØ±Ø§Ù† Ú©Ø¬Ø§Ø³ØªØŸ", # Out-of-domain question
            "expected_context_substring": ""
        }
    ]
    print(f"Loaded {len(eval_dataset)} evaluation cases.")

    # --- 2. Initialize the Evaluators ---
    faithfulness_evaluator = FaithfulnessEvaluator(llm=llm)
    relevancy_evaluator = RelevancyEvaluator(llm=llm)
    print("Evaluators (Faithfulness, Relevancy) initialized.")

    # --- 3. Run the Evaluation Loop ---
    print("\n" + "="*60)
    print("ğŸ”¬ Running Comprehensive Evaluations...")
    print("="*60 + "\n")
    
    retrieval_results = []
    faithfulness_results = []
    relevancy_results = []

    for i, case in enumerate(eval_dataset, 1):
        question = case["query"]
        expected_context_str = case["expected_context_substring"]
        
        print(f"--- Evaluating Case #{i}: {question} ---")

        # a. Retrieval Step
        retrieved_nodes = retriever.retrieve(question)
        retrieved_context_list = [node.get_content() for node in retrieved_nodes]
        retrieved_context_full_str = "\n\n".join(retrieved_context_list)

        # b. Retrieval Evaluation (Hit Rate)
        is_hit = expected_context_str in retrieved_context_full_str if expected_context_str else not retrieved_context_full_str
        retrieval_results.append(is_hit)
        print(f"ğŸ¯ Retrieval Result: {'HIT' if is_hit else 'MISS'}")

        # c. Generation Step
        prompt = f"Context: {retrieved_context_full_str}\n\nQuestion: {question}\n\nAnswer:"
        response_obj = llm.complete(prompt)
        response_text = response_obj.text
        print(f"ğŸ’¬ Generated Response: {response_text}")

        # --- d. Response Evaluation (Using the correct 'evaluate' method) ---
        
        # Faithfulness Evaluation
        faithfulness_result = faithfulness_evaluator.evaluate(
            response=response_text, 
            contexts=retrieved_context_list # Pass the list of context strings
        )
        faithfulness_results.append(faithfulness_result.passing)
        print(f"âš–ï¸ Faithfulness Result: {'PASS' if faithfulness_result.passing else 'FAIL'}")

        # Relevancy Evaluation
        relevancy_result = relevancy_evaluator.evaluate(
            query=question, 
            response=response_text, 
            contexts=retrieved_context_list # Pass the list of context strings
        )
        relevancy_results.append(relevancy_result.passing)
        print(f"ğŸ“ˆ Relevancy Result: {'PASS' if relevancy_result.passing else 'FAIL'}")
        
        print("-" * 60 + "\n")

    # --- 4. Print Final Aggregated Report ---
    print("="*60)
    print("ğŸ“Š Final Evaluation Report")
    print("="*60)
    
    retrieval_pass_rate = (sum(retrieval_results) / len(retrieval_results)) * 100
    print(f"Retrieval Hit Rate: {retrieval_pass_rate:.2f}%")
    
    faithfulness_pass_rate = (sum(faithfulness_results) / len(faithfulness_results)) * 100
    print(f"Faithfulness Pass Rate: {faithfulness_pass_rate:.2f}%")
    
    relevancy_pass_rate = (sum(relevancy_results) / len(relevancy_results)) * 100
    print(f"Relevancy Pass Rate: {relevancy_pass_rate:.2f}%")
    
    print("="*60)

if __name__ == "__main__":
    main()
